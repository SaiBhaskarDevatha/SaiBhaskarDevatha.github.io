<html>
	<head>
		<title>Music Genré Classification</title>
		<link rel="shortcut icon" href="Pictures/favicon.png">
		<link rel="stylesheet" type="text/css" href="style.css">
		<!-- Font -->
			<link href='https://fonts.googleapis.com/css?family=Work+Sans:400,700,600,500,300,200' rel='stylesheet' type='text/css'>
		<!-- Open Graph -->
		<meta property="og:image" content="https://image.ibb.co/kOLoyR/dsp.jpg" />
		<meta property="og:image:type" content="image/jpeg" />
		<meta property="og:image:alt" content="Music Genré Classification">
		<meta property="og:type" content="Website">
		<meta property="og:url" content="http://www.saibhaskardevatha.co.in/projects/dsp/">
		<meta property="og:title" content="Music Genré Classification">
	</head>
	<body>
		<div class="container">
			<div class="title" name="top">Music Genré Classification</div>
			<div class="authors">

				<!-- Start edit here  -->
				<p>Deekshith Raya, 150102069, ECE</p> &nbsp; &nbsp;
				<p>Vinod Patidar, 150102070, ECE</p> &nbsp; &nbsp;
				<p>Yash Khatri, 150102071, ECE</p> &nbsp; &nbsp;
				<p>Sai Bhaskar Devatha, 150108012, EEE</p>
				<!-- Stop edit here -->

			</div>

			<div class="section" id="abs">
				<div class="heading">Abstract</div>
				<div class="text">

					<!-- Start edit here  -->
					Harmonic and melodic characteristics of a music are the ones that make a sound feel like music. Thus, making them a key distinguisher for an automatic music type classifier like ours. Pitch is a perceptual property of sound that makes humans distinguish between music based on these characteristics. Chroma, a key-component of pitch, is widely used in measuring and analysing these characteristics due to close correlation, while being robust to changes in instrumentation and timbre of the music. Therefore, in this project we extracted and analysed chromagram to study their effectiveness in classifying music into four broad categories namely – Classical, Jazz, Metal and Pop.
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading" id="intro">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->
					With day-by-day increasing internet penetration, huge amount of useful data is available at proximity to people. Although it seems that there is ease of access to data, but this exponentially increasing amount of data brings to table a new problem – most of this chunk is unclassified.<br><br>
					Through this project, we aim to resolve this problem with something very close to people – music. We aim to explore various methodologies used to develop an automatic music genre classifier and thus, help in comparing efficiency to these methods.<br><br>
					Apart from the generic use of classification, it can be further used to better understand audio properties and human perception of music. Moreover, its applications can be extended to develop various systems like music genre-based disco lights and emotion-mapped music. 
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading">1.1 Introduction to Problem</div>
					<div class="text">

						<!-- Start edit here  -->
						While building such a classifier, the major challenge lies in deciding the frame rate and in feature computation and extraction. <br><br>
					    The decision of the frame rate is extremely critical to the performance of the classifier due to huge variations with time in a music file i.e. the audio might lean more toward one genre at the start and toward another at the end, but the average is that matters the most. <br><br>
					    On similar lines, extraction of features which detect subtle differences between the genres are very important to enhance the accuracy of the classifier.
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">1.2 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
						<a href="Pictures/example.jpg" target="_blank"><img src="Pictures/example.jpg" alt="This text displays when the image is umavailable"/></a>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						The field of automatic music genre classification has been of peak interest to audio signal processing researchers for quite some time. There are various publications targeting different set of features and using different statistical methodologies for classification. <br><br>
						George Tzanetakis, Georg Essl and Perry Cook pioneered in the field and published their work on 9-dimensional feature vector: mean-Centroid, mean-Rolloff, mean-Flux, mean-Zero Crossings, std-Centroid, std-Rolloff, std-Flux, std-Zero Crossings, Low Enegry. They obtained an accuracy of 16 % and 62% with random and Gaussian classifier. Hariharan Subramanian used MFCC, Rhythmic features and MPEG-7 features in addition to the above features. In Music Genre Classification, Michael Haggblade, Yang Hong and Kenny Kao used MFCC and got an accuracy of around 87% with DAG SVM. In Music type classification by Spectral Contrast feature, an accuracy of 82.3% was observed.<sup><a href="#1" style="text-decoration: none;">[1]</a></sup> <br><br>
						Juan Pablo Bello describes in his paper that most music is based on the tonality system i.e sounds arranged according to pitch relationships into interdependent spatial and temporal structures. And also shows usage of chroma to analyze these relations and therefore understand human perception of music. This inspired us to experiment with chromagram in our automatic music genre classification project.<sup><a href="#8" style="text-decoration: none;">[8]</a></sup>

						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.4 Proposed Approach</div>
					<div class="text">

						<!-- Start edit here  -->
						The automatic music genre classifier, here, is aimed to categorize a musical data into 4 broad categories – Classical, Jazz, Metal and Pop. The categorization is done by using a classifier upon a vector of features computed from the musical data. <br><br>
						Humans are remarkably good at genre classification as they can identify genre of a music by 250 milliseconds of an audio. This suggests that genre classification methodology should be as close as possible to human perception of music rather than any higher-level theoretical description. Therefore, here we have used chroma-based features as they are closely correlated to harmonic and melodic aspects of music, while being robust to changes in timbre and instrumentation. Chromagram is also very widely used to analyze and map human perception of music to signal processing techniques. Further, we compared performance of these chroma-based audio features or pitch class profiles with the performance of other features like MFCC, zero-crossing, rhythm based features, etc to establish classification efficiency associated with each.

						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.5 Report Organization</div>
					<div class="text">

						<!-- Start edit here  -->
						The report is organized as follows, <br>
						1. Title and Authors.<sup><a href="#top" style="text-decoration: none;">[Top]</a></sup> <br>
						2. Abstract.<sup><a href="#abs" style="text-decoration: none;">[0]</a></sup> <br>
						3. Introduction to Problem.<sup><a href="#intro" style="text-decoration: none;">[1]</a></sup> <br>
						4. Theory.<sup><a href="#theory" style="text-decoration: none;">[2]</a></sup> <br>
						5. Experimental Results and Discussions.<sup><a href="#results" style="text-decoration: none;">[3]</a></sup> <br>
						6. Conclusions and Summary.<sup><a href="#summary" style="text-decoration: none;">[4]</a></sup> <br>
						7. References.<sup><a href="#1" style="text-decoration: none;">[5]</a></sup>  <br>
						The above order is followed to ensure every finding of us is well documented.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section" id="theory">
				<div class="heading">2. Proposed Approach</div>
				<div class="text">
					<!-- Start edit here  -->
					The approach used here to build this automatic music genre classifier involves three major steps - <br>
					1. Data Preprocessing <br>
					2. Feature Extraction and Selection <br>
					3. Classification <br>
					<!-- Stop edit here -->
				</div>

				<div class="subsection">
					<div class="heading">2.1 Data Preprocessing</div>
					<div class="text">
						<!-- Start edit here  -->
						Before diving right into feature analysis and classification, one needs to have an appropriate format of analysable data. The GTZAN dataset has data in the ".au" format. This was converted into ".wav" format using <a href="https://www.anymp4.com/audio-converter/">AnyMP4 Audio Converter</a> software. Further, scipy.io.wavefile library was used to convert the ".wav" form data into a time series and store as a Numpy array. This same Numpy array was then used for feature analysis and classification purposes.
						<!-- Stop edit here -->
					</div>
				</div>

			</div>

			<div class="section" id="results">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="heading">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						<a href="http://marsyasweb.appspot.com/download/data_sets/" target="_blank">GTZAN Dataset</a> is considered as a standard dataset for music genre classification. This dataset was made and used by G. Tzanetakis and P. Cook, who were pioneers in the music genre classification problem.<sup><a href="#1" style="text-decoration: none;">[1]</a></sup> The dataset consists of 400 audio tracks each 30 seconds long. It contains 4 genres, each represented by 100 tracks. The tracks are all 22050Hz Mono 16-bit audio files in .au format.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						To be updated soon.
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">3.3 Results</div>
					<div class="text">

						<!-- Start edit here  -->
						To be updated soon.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section" id="summary">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="heading">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						To be updated soon.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						Further, we look forward to include more features into the application and improvise our classification algorithm to improve overall performance. We also plan to broaden the spectrum of the genres used.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.3 Applications</div>
					<div class="text">

						<!-- Start edit here  -->
						Apart from the most generic use of classifying huge chunks of data, this classifier can also be used for following applications, <br>
						1. Developing an automatic genre based disco lights system. <br>
						2. Automatic Equaliser. <br>
						3. Emotion-mapped music player.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">5. References</div>
				<div class="text" style="font-size: 14px;">

					<!-- Start edit here  -->
					<p id="1"><a href="http://ismir2001.ismir.net/pdf/tzanetakis.pdf" target="_blank">[1]</a> George Tzanetakis, Georg Essl and Perry Cook, Automatic Musical Genre Classification of Audio Signals.</p><br>
					<p id="2"><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035731&tag=1" target="_blank">[2]</a> MDan-Ning hang, Lie Lu, Hong-Jiang Zhang, Jian-Hua Tao and Lian-Hong Cui, Music type classification by Spectral Contrast feature.</p><br>
					<p id="3"><a href="https://pdfs.semanticscholar.org/3b05/d8762acf58a2a55e3520a7ccb84d673f8f7d.pdf" target="_blank">[3]</a> Hariharan Subramanian, Audio Signal Classification.</p><br>
					<p id="4"><a href="http://cs229.stanford.edu/proj2011/HaggbladeHongKao-MusicGenreClassification.pdf" target="_blank">[4]</a> Michael Haggblade, Yang Hong and Kenny Kao, Music Genre Classification.</p><br>
					<p id="5"><a href="https://pdfs.semanticscholar.org/31cf/b91158031038ad16218997cc9724a9ab5c66.pdf" target="_blank">[5]</a> Meinard M¨uller and Sebastian Ewert, Chroma Toolbox: MATLAB Implementations for extracting variants of Chroma-based audio features.</p><br>
					<p id="6"><a href="https://infoscience.epfl.ch/record/33926/files/Vetterli87.pdf" target="_blank">[6]</a> Martin Vetterli, A Theory of Multirate Filter Banks.</p><br>
					<p id="7"><a href="http://music.psych.cornell.edu/articles/reviews/A_theory_of_tonal_hierarchies_in_music.pdf" target="_blank">[7]</a> Carol L. Krumhansl and Lola L. Cuddy, A Theory of Tonal Hierarchies in Music.</p><br>
					<p id="8"><a href="http://www.nyu.edu/classes/bello/MIR_files/tonality.pdf" target="_blank">[8]</a> Juan Pablo Bello, Chroma and tonality, Music Information Retrieval.</p><br>
					<p id="9"><a href="http://www.flipcode.com/misc/BeatDetectionAlgorithms.pdf" target="_blank">[9]</a> Fredric Patin, Beat Detection Algorithms.</p><br>
				</div>
			</div>

			<div id="back"><p><a href="../../index.html">Báck</a></p></div>
		</div>
	</body>
</html>
