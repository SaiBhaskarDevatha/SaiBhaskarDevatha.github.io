<html>
	<head>
		<title>Music Genré Classification</title>
		<link rel="shortcut icon" href="Pictures/favicon.png">
		<link rel="stylesheet" type="text/css" href="style.css">
		<!-- Font -->
			<link href='https://fonts.googleapis.com/css?family=Work+Sans:400,700,600,500,300,200' rel='stylesheet' type='text/css'>
		<!-- Open Graph -->
		<meta property="og:image" content="https://image.ibb.co/kOLoyR/dsp.jpg" />
		<meta property="og:image:type" content="image/jpeg" />
		<meta property="og:image:alt" content="Music Genré Classification">
		<meta property="og:type" content="Website">
		<meta property="og:url" content="http://www.saibhaskardevatha.co.in/projects/dsp/">
		<meta property="og:title" content="Music Genré Classification">
	</head>
	<body>
		<div class="container">
			<div class="title" name="top">Music Genré Classification</div>
			<div class="authors">

				<!-- Start edit here  -->
				<p>Deekshith Raya, 150102069, ECE</p> &nbsp; &nbsp;
				<p>Vinod Patidar, 150102070, ECE</p> &nbsp; &nbsp;
				<p>Yash Khatri, 150102071, ECE</p> &nbsp; &nbsp;
				<p>Sai Bhaskar Devatha, 150108012, EEE</p>
				<!-- Stop edit here -->

			</div>

			<div class="section" id="abs">
				<div class="heading">Abstract</div>
				<div class="text">

					<!-- Start edit here  -->
					Harmonic and melodic characteristics of a music are the ones that make a sound feel like music. Thus, making them a key distinguisher for an automatic music type classifier like ours. Pitch is a perceptual property of sound that makes humans distinguish between music based on these characteristics. Chroma, a key-component of pitch, is widely used in measuring and analysing these characteristics due to close correlation, while being robust to changes in instrumentation and timbre of the music. Therefore, in this project we extracted and analysed chromagram to study their effectiveness in classifying music into four broad categories namely – Classical, Jazz, Metal and Pop.
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading" id="intro">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->
					With day-by-day increasing internet penetration, huge amount of useful data is available at proximity to people. Although it seems that there is ease of access to data, but this exponentially increasing amount of data brings to table a new problem – most of this chunk is unclassified.<br><br>
					Through this project, we aim to resolve this problem with something very close to people – music. We aim to explore various methodologies used to develop an automatic music genre classifier and thus, help in comparing efficiency to these methods.<br><br>
					Apart from the generic use of classification, it can be further used to better understand audio properties and human perception of music. Moreover, its applications can be extended to develop various systems like music genre-based disco lights and emotion-mapped music. 
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading">1.1 Introduction to Problem</div>
					<div class="text">

						<!-- Start edit here  -->
						While building such a classifier, the major challenge lies in deciding the frame rate and in feature computation and extraction. <br><br>
					    The decision of the frame rate is extremely critical to the performance of the classifier due to huge variations with time in a music file i.e. the audio might lean more toward one genre at the start and toward another at the end, but the average is that matters the most. <br><br>
					    On similar lines, extraction of features which detect subtle differences between the genres are very important to enhance the accuracy of the classifier.
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">1.2 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
						<a href="Pictures/example.jpg" target="_blank"><img src="Pictures/example.jpg" alt="This text displays when the image is unavailable"/></a>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						The field of automatic music genre classification has been of peak interest to audio signal processing researchers for quite some time. There are various publications targeting different set of features and using different statistical methodologies for classification. <br><br>
						George Tzanetakis, Georg Essl and Perry Cook pioneered in the field and published their work on 9-dimensional feature vector: mean-Centroid, mean-Rolloff, mean-Flux, mean-Zero Crossings, std-Centroid, std-Rolloff, std-Flux, std-Zero Crossings, Low Enegry. They obtained an accuracy of 16 % and 62% with random and Gaussian classifier.<sup><a href="#1">[1]</a></sup> Hariharan Subramanian used MFCC, Rhythmic features and MPEG-7 features in addition to the above features.<sup><a href="#3">[3]</a></sup> In Music Genre Classification, Michael Haggblade, Yang Hong and Kenny Kao used MFCC and got an accuracy of around 87% with DAG SVM.<sup><a href="#4">[4]</a></sup> In Music type classification by Spectral Contrast feature, an accuracy of 82.3% was observed. <br><br>
						Juan Pablo Bello describes in his paper that most music is based on the tonality system i.e sounds arranged according to pitch relationships into interdependent spatial and temporal structures. And also shows usage of chroma to analyze these relations and therefore understand human perception of music. This inspired us to experiment with chromagram in our automatic music genre classification project.<sup><a href="#8">[8]</a></sup>

						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.4 Proposed Approach</div>
					<div class="text">

						<!-- Start edit here  -->
						The automatic music genre classifier, here, is aimed to categorize a musical data into 4 broad categories – Classical, Jazz, Metal and Pop. The categorization is done by using a classifier upon a vector of features computed from the musical data. <br><br>
						Humans are remarkably good at genre classification as they can identify genre of a music by 250 milliseconds of an audio. This suggests that genre classification methodology should be as close as possible to human perception of music rather than any higher-level theoretical description. Therefore, here we have used chroma-based features as they are closely correlated to harmonic and melodic aspects of music, while being robust to changes in timbre and instrumentation. Chromagram is also very widely used to analyze and map human perception of music to signal processing techniques. Further, we compared performance of these chroma-based audio features or pitch class profiles with the performance of other features like MFCC, zero-crossing, rhythm based features, etc to establish classification efficiency associated with each.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.5 Report Organization</div>
					<div class="text">

						<!-- Start edit here  -->
						The report is organized as follows, <br>
						1. Title and Authors.<sup><a href="#top">[Top]</a></sup> <br>
						2. Abstract.<sup><a href="#abs">[0]</a></sup> <br>
						3. Introduction to Problem.<sup><a href="#intro">[1]</a></sup> <br>
						4. Proposed Approach.<sup><a href="#theory">[2]</a></sup> <br>
						5. Experimental Results, Project Code and Discussions.<sup><a href="#results">[3]</a></sup> <br>
						6. Conclusions and Summary.<sup><a href="#summary">[4]</a></sup> <br>
						7. References.<sup><a href="#1">[5]</a></sup>  <br>
						The above order is followed to ensure every finding of us is properly documented.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section" id="theory">
				<div class="heading">2. Proposed Approach</div>
				<div class="text">
					<!-- Start edit here  -->
					The approach used here to build this automatic music genre classifier involves three major steps - <br>
					&nbsp; &nbsp;2.1 Data Preprocessing <br>
					&nbsp; &nbsp;2.2 Feature Extraction and Selection <br>
					&nbsp; &nbsp;2.3 Classification <br>
					<!-- Stop edit here -->
				</div>

				<div class="subsection">
					<div class="heading">2.1 Data Preprocessing</div>
					<div class="text">
						<!-- Start edit here  -->
						Before diving right into feature analysis and classification, one needs to have an appropriate format of analysable data. The GTZAN dataset has data in the ".au" format. This was converted into ".wav" format using <a href="https://www.anymp4.com/audio-converter/" target="_blank">AnyMP4 Audio Converter</a> software. Further, scipy.io.wavefile library was used to convert the ".wav" form data into a time series and store as a Numpy array. This same Numpy array was then used for feature analysis and classification purposes.
						<!-- Stop edit here -->
					</div>
				</div>

				<div class="subsection">
					<div class="heading">2.2 Feature Extraction and Selection</div>
					<div class="text">
						<!-- Start edit here  -->
						The audio file, which was stored into the Numpy array, is then divided into equally sized frames for further analysis. The following procedure was then used to extract chroma-based features,<br>
						->&nbsp;&nbsp;Short Time Fourier Transform (STFT) was initially calculated for each frame. <br>
						->&nbsp;&nbsp;This was then passed through a chroma filter to obtain chromagram. <br>
						This chromagram was used to calculate features like centroid chroma, chroma spread, Min-Max chroma and chroma flux. In an additional approach, 10 frames were used together to calculate mean energy and standard deviation of each chroma. <br><br>

						<div class="image">

						<!-- Start edit here  -->
						<a href="Pictures/chromograms.jpg" target="_blank"><img src="Pictures/chromograms.jpg" alt="This text displays when the image is unavailable"/></a>
						<!-- Stop edit here -->
						<br>
					</div>

						<span id="inch">Chromagram Basics</span>: Most music is based on the tonality system. Tonality arranges sounds according to pitch relationships into interdependent spatial and temporal structures. Characterizing chords, keys, melody, motifs and even form, largely depends on these structures. The pitch helix is a representation of pitch relationships that places tones in the surface of a cylinder. It models the special relationship that exists between octave intervals. Chroma represents the inherent circularity of pitch organization. Chroma describes the angle of pitch rotation as it traverses the helix. Two octave-related pitches will share the same angle in the chroma circle: a relation that is not captured by a linear pitch scale (or even Mel). For the analysis of tonal music we quantize this angle into 12 positions or pitch classes. 

						<!-- Stop edit here -->
					</div>
					<div class="image">

						<!-- Start edit here  -->
						<a href="Pictures/helix.png" target="_blank"><img src="Pictures/helix.png" style="height: 100%;width: auto;" alt="This text displays when the image is unavailable"/></a>  <figcaption style="text-align: center;">Chroma can be taught as a pitch of frequencies</figcaption>

						<a href="Pictures/filter.png" target="_blank"><img src="Pictures/filter.png" alt="This text displays when the image is unavailable"/></a><figcaption style="text-align: center;">Chroma filter bank</figcaption>
						<!-- Stop edit here -->

					</div>
					<div class="text">
						<!-- Start edit here  --> <br>
						<span id="inch">Chroma Centroid</span>: It is the weighted mean of chroma bins where the weights are the energy associated with chroma bins. <br><br>
						<span id="inch">Chroma spread</span>: It gives an idea of the shape of chroma in each frame. Its calculated as follows. <br><br>
						<span id="inch">Max Chroma</span>: The Chroma bin associated with maximum energy. <br><br>
						<span id="inch">Min Chroma</span>: It is the least energy chroma bin.<br><br>
						<span id="inch">Chroma Flux</span>: It shows how chroma distribution varies across frames. It can be calculated as follows.
						<!-- Stop edit here -->
					</div>
						
				</div>

				<div class="subsection">
					<div class="heading">2.3 Classification</div>
					<div class="text">
						<!-- Start edit here  -->
						Two different classifiers, namely SVM and Multi-Layer Perceptron, were used upon the feature vectors that were extracted in the previous step. In SVM classifier, gamma was varied to obtain different levels of fitness. For Multi-Layer Perceptron, the following 3 configurations of hidden layers were used 8-8, 12-8-6, 3-3.
						<!-- Stop edit here -->
					</div>
				</div>

			</div>

			<div class="section" id="results">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="heading">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						<a href="http://marsyasweb.appspot.com/download/data_sets/" target="_blank">GTZAN Dataset</a> is considered as a standard dataset for music genre classification. This dataset was made and used by G. Tzanetakis and P. Cook, who were pioneers in the music genre classification problem.<sup><a href="#1">[1]</a></sup> The dataset consists of 400 audio tracks each 30 seconds long. It contains 4 genres, each represented by 100 tracks. The tracks are all 22050Hz Mono 16-bit audio files in .au format.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						Inspired from the features like spectral spread and centroid, generally used to analyse speech using spectral features, we tried extracting the parallel features in  chromogram domain. <br><br>
						In the initial attempt we constructed the feature vectors, on which the data was trained, has  following features: <br>
						&nbsp;&nbsp;1. Chroma Centroid <br>
						&nbsp;&nbsp;2. Chroma Spread <br>
						&nbsp;&nbsp;3. Min chroma <br>
						&nbsp;&nbsp;4. Max chroma <br>
						&nbsp;&nbsp;5. Chroma flux <br>
						These 5 features were augmented with the 12 chormagram coefficients obtained directly from chromogram making a 17 dimensional feature vector. <br><br>
						Not satisfied with the classification accuracy we increased our aperture of analysis by considering 10 frames with a hop length of 5 frames. We found the mean of each chroma coefficient and its variance over 10 frames thereby in a way observing chroma distribution this gave us 24 dimension feature vector which gave us improved results. 

						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.3 Project Code</div>
					<div class="text">

						<!-- Start edit here  -->
						All experiment codes for our project can be found from this <a href="https://github.com/SaiBhaskarDevatha/Music-Genre-Classification">GitHub Repository</a>.	
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.4 Results</div>
					<div class="text">
						<!-- Start edit here  -->
						<table style="width:100%;">
						<tr>
						    <th>Classes</th>
						    <th>Features</th> 
						    <th>Frame Size(No.of Frames)</th>
						    <th>Classifier</th>
						    <th>Accuracy</th>
						</tr>
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>Cen, Var, Max, Min, Flux</td>
						    <td>0.046s(2048)</td>
						    <td>SVM</td>
						    <td>38.8%</td>
						</tr>
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>12 Chroma Coefficients</td>
						    <td>0.046s(2048)</td>
						    <td>SVM</td>
						    <td>42.5%</td>
						</tr>
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>Cen, Var, Max, Min, Flux, 12 Chroma Coefficients</td>
						    <td>0.046s(2048)</td>
						    <td>SVM</td>
						    <td>43.5%</td>
						</tr>
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>SVM</td>
						    <td>46.5%</td>
						</tr>
						
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>SVM</td>
						    <td>54.5%</td>
						</tr>
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>SVM Overfit(Gamma = 5)</td>
						    <td>55.4%</td>
						</tr>	
						<tr>
						    <td>Rock, Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>MLP(12,8,6)</td>
						    <td>57%</td>
						</tr>
						<tr>
						    <td>Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>MLP(8,8)</td>
						    <td>68.5%</td>
						</tr>
						<tr>
						    <td>Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>SVM</td>
						    <td>68.6%</td>
						</tr>
						<tr>
						    <td>Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>MLP(6,3,3)</td>
						    <td>68.8%</td>
						</tr>

						<tr>
						    <td>Classical, Jazz, Metal</td>
						    <td>Mean and Variance of 12 Chroma Coefficients</td>
						    <td>0.232s(4096)</td>
						    <td>MLP(12,8,6)</td>
						    <td>70.1%</td>
						</tr>
						</table>		
						<!-- Stop edit here -->
					</div>
				</div>
			</div>

			<div class="section" id="summary">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="heading">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						This project was aimed to tackle the problem of automatic music genre classification based on various features. We pre-processed the data first followed by feature extraction and selection, lastly followed by classification. Here, we focussed our spectrum of features onto just chroma-based features as these act as a good metric for human perception of music. Through feature analysis and classification, a maximum accuracy of 70% was obtained.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						Further, we look forward to include more features into the application and improvise our classification algorithm to improve overall performance. We also plan to broaden the spectrum of the genres used.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.3 Applications</div>
					<div class="text">

						<!-- Start edit here  -->
						Apart from the most generic use of classifying huge chunks of data, this classifier can also be used for following applications, <br>
						&nbsp;&nbsp;1. Developing an automatic genre based disco lights system. <br>
						&nbsp;&nbsp;2. Automatic Equaliser. <br>
						&nbsp;&nbsp;3. Emotion-mapped music player.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">5. References</div>
				<div class="text" style="font-size: 14px;">

					<!-- Start edit here  -->
					<p id="1"><a href="http://ismir2001.ismir.net/pdf/tzanetakis.pdf" target="_blank">[1]</a> George Tzanetakis, Georg Essl and Perry Cook, Automatic Musical Genre Classification of Audio Signals.</p><br>
					<p id="2"><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035731&tag=1" target="_blank">[2]</a> MDan-Ning hang, Lie Lu, Hong-Jiang Zhang, Jian-Hua Tao and Lian-Hong Cui, Music type classification by Spectral Contrast feature.</p><br>
					<p id="3"><a href="https://pdfs.semanticscholar.org/3b05/d8762acf58a2a55e3520a7ccb84d673f8f7d.pdf" target="_blank">[3]</a> Hariharan Subramanian, Audio Signal Classification.</p><br>
					<p id="4"><a href="http://cs229.stanford.edu/proj2011/HaggbladeHongKao-MusicGenreClassification.pdf" target="_blank">[4]</a> Michael Haggblade, Yang Hong and Kenny Kao, Music Genre Classification.</p><br>
					<p id="5"><a href="https://pdfs.semanticscholar.org/31cf/b91158031038ad16218997cc9724a9ab5c66.pdf" target="_blank">[5]</a> Meinard M¨uller and Sebastian Ewert, Chroma Toolbox: MATLAB Implementations for extracting variants of Chroma-based audio features.</p><br>
					<p id="6"><a href="https://infoscience.epfl.ch/record/33926/files/Vetterli87.pdf" target="_blank">[6]</a> Martin Vetterli, A Theory of Multirate Filter Banks.</p><br>
					<p id="7"><a href="http://music.psych.cornell.edu/articles/reviews/A_theory_of_tonal_hierarchies_in_music.pdf" target="_blank">[7]</a> Carol L. Krumhansl and Lola L. Cuddy, A Theory of Tonal Hierarchies in Music.</p><br>
					<p id="8"><a href="http://www.nyu.edu/classes/bello/MIR_files/tonality.pdf" target="_blank">[8]</a> Juan Pablo Bello, Chroma and tonality, Music Information Retrieval.</p><br>
					<p id="9"><a href="http://www.flipcode.com/misc/BeatDetectionAlgorithms.pdf" target="_blank">[9]</a> Fredric Patin, Beat Detection Algorithms.</p><br>
					<p id="10"><a href="http://www.justinsalamon.com/uploads/4/3/9/4/4394963/justin_salamon_-_masterthesis.pdf" target="_blank">[10]</a> Justin Jonathan Salamon, Chroma-based Predominant Melody and Bass Line Extraction from Music Audio Signals</p><br>
				</div>
			</div>

			<div id="back"><p><a href="../../index.html">Báck</a></p></div>
		</div>
	</body>
</html>
